{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww18140\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs30 \cf0 Notes on assignment 1\
\

\b0\fs24 In this assignment, all tasks are very well driven. Most parts of the code are copy/paste from udacity page. 
\i May need some cleaning.
\i0 \
Here are a few comments/results concerning most important parts of the assignment.\
\
- Computing \'ab\'a0overlapping samples\'a0\'bb in problem 5 should be done using hash function.\
- Sanitized dataset (with only unique samples) is saved under\
\

\b\fs26 Some results on the logisticRegression classifier\

\b0\fs24 \
The model used is created with following command. I actually made a copy/paste from sklearn website to choose all these parameters.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\b \cf0 * Training size = 50\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\b0 \cf0 \
\

\b * Training size = 100\

\b0 \
 precision    recall  f1-score   support\
\
          0       0.77      0.50      0.61      1000\
          1       0.86      0.68      0.76      1000\
          2       0.87      0.66      0.75      1000\
          3       0.69      0.77      0.73      1000\
          4       0.67      0.74      0.71      1000\
          5       0.63      0.82      0.71      1000\
          6       0.77      0.69      0.73      1000\
          7       0.64      0.75      0.69      1000\
          8       0.64      0.66      0.65      1000\
          9       0.64      0.76      0.69      1000\
\
avg / total      
\b  0.72      0.70      0.70     10000
\b0 \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\b \cf0 * Training size = 1000\
\

\b0 Classification report : \
             precision    recall  f1-score   support\
\
          0       0.84      0.76      0.80      1000\
          1       0.77      0.74      0.76      1000\
          2       0.76      0.78      0.77      1000\
          3       0.81      0.77      0.79      1000\
          4       0.80      0.76      0.78      1000\
          5       0.84      0.82      0.83      1000\
          6       0.85      0.72      0.78      1000\
          7       0.72      0.82      0.76      1000\
          8       0.66      0.75      0.70      1000\
          9       0.75      0.84      0.79      1000\
\
avg / total       
\b 0.78      0.77      0.78     10000
\b0 \

\b \
* Training size = 5000\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\b0 \cf0 Classification report : \
             precision    recall  f1-score   support\
\
          0       0.79      0.79      0.79      1000\
          1       0.79      0.74      0.76      1000\
          2       0.79      0.80      0.80      1000\
          3       0.81      0.77      0.79      1000\
          4       0.77      0.74      0.75      1000\
          5       0.79      0.81      0.80      1000\
          6       0.75      0.75      0.75      1000\
          7       0.82      0.81      0.81      1000\
          8       0.71      0.76      0.74      1000\
          9       0.77      0.82      0.79      1000\
\
avg / total       
\b 0.78      0.78      0.78     10000\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 * Whole training dataset\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \

\b0         precision    recall  f1-score   support\
\
          0       0.88      0.85      0.87      1000\
          1       0.85      0.83      0.84      1000\
          2       0.85      0.85      0.85      1000\
          3       0.86      0.84      0.85      1000\
          4       0.85      0.80      0.83      1000\
          5       0.84      0.88      0.86      1000\
          6       0.83      0.81      0.82      1000\
          7       0.84      0.87      0.85      1000\
          8       0.79      0.79      0.79      1000\
          9       0.81      0.87      0.84      1000
\b \
\
avg / total       0.84      0.84      0.84     10000\

\b0 \
\
Not that it is not too bad for an off-the-shelf model! However we need to be careful since we found out that there were overlapping in datasets.\
Simulations show that duplicate do not really affect the results when training size is big enough (>1000 samples).\
}